{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "from llama_index import LLMPredictor, ServiceContext, SQLDatabase, GPTVectorStoreIndex\n",
    "from llama_index.indices.struct_store import SQLTableRetrieverQueryEngine\n",
    "from llama_index.objects import SQLTableNodeMapping, ObjectIndex, SQLTableSchema\n",
    "\n",
    "from common.constants import OPENAI_API_KEY, SQL_CONNECTION\n",
    "\n",
    "from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex, LLMPredictor, PromptHelper, StorageContext, \\\n",
    "    load_index_from_storage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import gradio as gr\n",
    "\n",
    "import common.chat_bot_helper as chat_bot_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b59516",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_uri = SQL_CONNECTION\n",
    "engine = create_engine(pg_uri)\n",
    "\n",
    "# load all table definitions\n",
    "metadata_obj = MetaData()\n",
    "metadata_obj.reflect(engine)\n",
    "\n",
    "sql_database = SQLDatabase(engine)\n",
    "\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "\n",
    "table_schema_objs = []\n",
    "for table_name in metadata_obj.tables.keys():\n",
    "    table_schema_objs.append(SQLTableSchema(table_name=table_name))\n",
    "\n",
    "# We dump the table schema information into a vector index. The vector index is stored within the context builder for future use.\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    table_schema_objs,\n",
    "    table_node_mapping,\n",
    "    GPTVectorStoreIndex,\n",
    ")\n",
    "service_context = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_index(directory_path):\n",
    "    # model params\n",
    "    # max_input_size: maximum size of input text for the model.\n",
    "    # num_outputs: number of output tokens to generate.\n",
    "    # max_chunk_overlap: maximum overlap allowed between text chunks.\n",
    "    # chunk_size_limit: limit on the size of each text chunk.\n",
    "    max_input_size = 4096\n",
    "    num_outputs = 512\n",
    "    chunk_size_limit = 600\n",
    "\n",
    "    # llm predictor with langchain ChatOpenAI\n",
    "    # ChatOpenAI model is a part of the LangChain library and is used to interact with the GPT-3.5-turbo model provided by OpenAI\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, chunk_overlap_ratio=0.1,\n",
    "                                 chunk_size_limit=chunk_size_limit)\n",
    "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
    "\n",
    "    # SQL\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "    # read documents from docs folder\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "    # init index with documents data\n",
    "    # This index is created using the LlamaIndex library. It processes the document content and constructs the index to facilitate efficient querying\n",
    "    # service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "    # index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    index = GPTVectorStoreIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "    # save the created index\n",
    "    # index.save_to_disk('index.json')\n",
    "    index.storage_context.persist('./vector-store-index')\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct SQLTableRetrieverQueryEngine. \n",
    "# Passing in the ObjectRetriever so that we can dynamically retrieve the table during query-time.\n",
    "\n",
    "query_engine_sql = SQLTableRetrieverQueryEngine(\n",
    "    sql_database,\n",
    "    obj_index.as_retriever(similarity_top_k=1),\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(curr_user_comment, history):\n",
    "    jira_response = chat_bot_helper.check_response_if_jt_requested(history, curr_user_comment)\n",
    "    if jira_response is not None and jira_response['is_custom'] == True:\n",
    "        return jira_response['message']\n",
    "\n",
    "    response = None\n",
    "\n",
    "    if chat_bot_helper.query_to_sql(curr_user_comment):\n",
    "        response = query_engine_sql.query(curr_user_comment)\n",
    "        print(\"SQL I ran to find the answer is :\\n \", response.metadata['sql_query'])\n",
    "    else:\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=\"./vector-store-index/\")\n",
    "        index = load_index_from_storage(storage_context)\n",
    "        query_engine = index.as_query_engine()\n",
    "        response = query_engine.query(curr_user_comment)\n",
    "\n",
    "    # Analizing current response to open a Jira Ticket:\n",
    "    jira_response = chat_bot_helper.ask_create_jira(response.response, curr_user_comment)\n",
    "    if jira_response is not None and jira_response['is_custom'] == True:\n",
    "        return jira_response['message']\n",
    "\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4754be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = init_index(\"docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5605dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set look and feel\n",
    "theme = gr.themes.Soft(\n",
    "    primary_hue=gr.themes.Color(c100=\"#fee2e2\", c200=\"#fecaca\", c300=\"#fca5a5\", c400=\"#f87171\", c50=\"#fef2f2\",\n",
    "                                c500=\"#a80025\", c600=\"#dc2626\", c700=\"#b91c1c\", c800=\"#991b1b\", c900=\"#7f1d1d\",\n",
    "                                c950=\"#6c1e1e\"),\n",
    "    text_size=gr.themes.sizes.text_sm,\n",
    "    spacing_size=gr.themes.sizes.spacing_sm,\n",
    "    radius_size=gr.themes.sizes.radius_lg,\n",
    ").set(\n",
    "    body_text_weight='300',\n",
    "    background_fill_primary='*primary_50'\n",
    ")\n",
    "\n",
    "botAvatar = gr.Chatbot(\n",
    "    [],\n",
    "    elem_id=\"chatbot\",\n",
    "    bubble_full_width=False,\n",
    "    sanitize_html=True,\n",
    "    avatar_images=(\n",
    "        (os.path.join(os.path.abspath(''), \"images/user.png\")),\n",
    "        (os.path.join(os.path.abspath(''), \"images/avatar.png\"))),\n",
    ")\n",
    "\n",
    "img = (os.path.join(os.path.abspath(''), \"./images/redqueen.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "descript = f\"\"\"\n",
    "        <div style=\"display: flex; align-items: center;\">\n",
    "            <img src=\"./images/redqueen.png\" alt=\"Red Queen's AI ChatBot\" style=\"width: 120px; margin-right: 30px;\"/>\n",
    "            <h3 style=\"padding-top: 15px;\">\n",
    "                Your AI Buddy\n",
    "            </h3>\n",
    "        </div>\n",
    "        <br>\n",
    "        <br>\n",
    "        Ask your questions...\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ui interface to interact with gpt-3 model\n",
    "iface = gr.ChatInterface(fn=chatbot,\n",
    "                         chatbot=botAvatar,\n",
    "                         description=descript,\n",
    "                         theme=theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7788853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.launch(allowed_paths=[\"images/avatar.png\", \"images/user.png\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
